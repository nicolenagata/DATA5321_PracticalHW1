---
title: "PracticalHW1:DecisionTrees"
output: html_document
date: "2025-04-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This homework will use decision trees to explore youth drug use using survey data from theÂ [National Survey on Drug Use and Health](https://www.datafiles.samhsa.gov/dataset/national-survey-drug-use-and-health-2020-nsduh-2020-ds0001). The survey collects detailed information on respondents' demographics, youth experiences, use of various drugs, and more. Because there are so many different data and categories available:

***Your task is to investigate factors that are correlated with youth drug use.***

In your models, you must include one example of each of the following problem types:

-   binary classification (e.g. has or has not used cigarettes)

-   multi-class classification (e.g. differentiate between seldom, sometimes, and frequent marijuana use)

-   regression (e.g. number of days per year a person has used alcohol)

## Problem Framework

***Consider factors that are correlated with youth marijuana use to suggest implementations for school drug use programs***

Model Implementations:

-   binary classification (has or has not used marijuana)

-   multi-class classification (edifferentiate between seldom, sometimes, and frequent marijuana use)

-   regression (average age first used marijuana)

## Data Cleaning and Exploration

```{r}
# load cleaned data
load("youth_data.Rdata")

# Define All Predictor Variables
vars <- c('IRSEX', 'NEWRACE2', 'HEALTH2', 'EDUSCHLGO', 'EDUSCHGRD2','EDUSKPCOM', 'IMOTHER', 'IFATHER', 'INCOME', 'GOVTPROG','POVERTY3', 'PDEN10', 'COUTYP4', 'MRJFLAG')
```

## Binary Classification - Ever Used Mrj

To understand factors that influence marijuana use in youths, we can explore predictors that tell us whether or not participants ever used marijuana from a binary classification tree. Predictor variables were chosen from different demographic and socio-economic factors, with similar predictors like attending school and grade level removed as to reduce correlation between predictors. There is also a heavy data imbalance, where most of the participants have never used marijuana. Since classifications trees are heavily influenced by training data, balancing data will help improve prediction accuracy.

The classification tree is fit on 70% of the balanced data set, and tested on 30% of the balanced data. The tree uses basically one predictor variable to determine whether or not a participant has ever used marijuana, if a student is 9th grade or below they have not used marijuana and above 9th grade they have. This simple classification model correctly predicts whether or not a student has used marijuana 64.02% of the time, but prediction is heavily dominated by a students grade level.

To reduce variance in the original model, a random forest approach would consider factors from multiple trees as well as limit the amount of variables considered at each split. By using a random forest approach, factors that affect drug use, other than grade level, could become apparent. This model does a much better job at predicting if participants ever used marijuana, with an increased accuracy of 69.87%. This model can be used to view important factors, but it is also important to note that variable importance identify how much a variable effects prediction, but does not show the direction they might affect the target variable (ie. positive or negative relationships), so we might use background research and logic to determine their impacts. Grade level continues to be an important predictor, suggesting that schools might want to have discussions around preventative drug use and awareness as early as possible. While grade level is still significantly an important predictor, other factors can be important like race and health. This could mean that youth drug awareness should also consider some socio-economic factors like targeting communities that are more exposed to drug usage and warning students with high health risks of potential repercussions.

```{r}
# install libaries
library(tree)
library(dplyr)
library(tidyr)

# Define variables
vars <- c('IRSEX', 'NEWRACE2', 'HEALTH2', 'EDUSCHGRD2', 'INCOME', 'IMOTHER', 'PDEN10', 'MRJFLAG')

# Subset and clean data
df_tree <- df %>%
  select(all_of(vars)) %>%
  drop_na() %>%
  mutate(MRJFLAG = as.factor(MRJFLAG))

# Data Imbalance 
table(df_tree$MRJFLAG)

# Underampling the majoirty class (0)
library(ROSE)
set.seed(123) 
df_balanced <- ovun.sample(MRJFLAG ~ ., data = df_tree, method = "both", N = 3362)$data
table(df_balanced$MRJFLAG)
```

```{r}
# train test split (70/30)
set.seed(123)
train_idx <- sample(1:nrow(df_balanced), nrow(df_balanced)*0.7)
train <- df_balanced[train_idx, ]
test <- df_balanced[-train_idx, ]

table(train$MRJFLAG)

# Fit classification tree
set.seed(123) 
mrj_tree <- tree(MRJFLAG ~ ., data = train)

# Print and plot the tree
summary(mrj_tree)
plot(mrj_tree)
text(mrj_tree, pretty = 0)

# predict on test
tree_pred <- predict(mrj_tree, newdata = test, type = "class")

# confusion matrix
table(Predicted = tree_pred, Actual = test$MRJFLAG)

# accuracy
mean(tree_pred == test$MRJFLAG)
```

```{r}
### Prune Tree
set.seed(123) 
cv_tree <- cv.tree(mrj_tree, FUN = prune.misclass)
plot(cv_tree$size, cv_tree$dev, type = "b", xlab = "Tree Size", ylab = "Misclassification")

# Prune to optimal size
best_size <- cv_tree$size[which.min(cv_tree$dev)]
pruned_tree <- prune.misclass(mrj_tree, best = best_size)

# Plot pruned tree
plot(pruned_tree)
text(pruned_tree, pretty = 0)

# Predict again
pruned_pred <- predict(pruned_tree, test, type = "class")
mean(pruned_pred == test$MRJFLAG)
```

```{r}
### Random Forest
# train test split
set.seed(123)
train_idx <- sample(1:nrow(df_balanced), nrow(df_balanced)*0.7)
train <- df_balanced[train_idx, ]
test <- df_balanced[-train_idx, ]

# fit random forest model
library(randomForest)
set.seed(123) 
rf_tree <- randomForest(MRJFLAG ~., data = train, mtry = 3, importance = TRUE)
rf_tree

# importance
importance(rf_tree)
varImpPlot(rf_tree)

# predict on test data
yhat.bag <- predict(rf_tree, newdata = test)
plot(yhat.bag, test$MRJFLAG) # plot points
abline(0, 1) # plot regression line

# accuracy
mean(yhat.bag == test$MRJFLAG)
```

## Multi-Class: \# days used marijuana in past month

Next, using a multi-class classification tree will help explore significant factors that are correlated with the number of days marijuana was used by youths in the past month. We will again use variables that are carefully selected from each category including demographics, school engagement, socio-economic status, family structure, and geographic context, as to avoid strong correlations among predictors.

There is a large data imbalance in our target variable, where most youths are non-marijuana users or have not used marijuana in the past month (5). To account for the disparity in the data, under sampling will help reduce the number of category 5 variables to match the number of 1-4 data points, while keeping enough data points to implement models like classification trees (reference to documentation used for under sampling technique: [link](https://www.rdocumentation.org/packages/UBL/versions/0.0.9/topics/RandUnderClassif)). The final balanced data set includes 745 observations, with each category consisting of about 150 data points.

```{r}
### DATA CLEANING MRJMDAYS
# install libaries
library(tree)

# Define variables
vars <- c('IRSEX', 'NEWRACE2', 'HEALTH2', 'EDUSCHGRD2', 'INCOME', 'IMOTHER', 'PDEN10', 'MRJMDAYS')

# Subset and clean data
df_tree <- df %>%
  select(all_of(vars)) %>%
  drop_na() %>%
  mutate(MRJMDAYS = as.factor(MRJMDAYS))

# Data Imbalance 
table(df_tree$MRJMDAYS)
  # 1: 1-2 Days
  # 2: 3-5 Days
  # 3: 6-19 Days
  # 4: 20-30 Days
  # 5: Non user or not in past month 
  # *** majority of youth falls under 5 category

# undersample the majority class (5)
library(UBL)
set.seed(123) 
df_balanced <- RandUnderClassif(MRJMDAYS ~ ., df_tree)
table(df_balanced$MRJMDAYS)
```

The following code fits a multi-class classification tree that predicts the number of days youths have used marijuana in the past month, using the specified predictor variables from before. The balanced data is split into a 70% training set that the model is fit to and a 30% test set where predictions are used to test the accuracy of the model.

Looking at the plot of the classification tree, we can see that the grade level is basically a deciding factor that predicts two extremes of marijuana usage within the past month: either a non-marijuana user/ has not used marijuana within the past month or has used a substantial amount (20-30 days of marijuana used) within the past month.

Looking at the confusion matrix and accuracy, this classification model is not picking up on the nuances of the data, even after the data has been under sampled. The model is biased towards predicting the two extremes, particularly more sensitive towards predicting no marijuana use for most youth. Furthermore, the accuracy is extremely low predicting the correct category of marijuana usage only about 24.5% of the time.

```{r}
### MULTI CLASSIFICATION: MRJMDAYS
# train test split (70/30)
set.seed(123)
train_idx <- sample(1:nrow(df_balanced), nrow(df_balanced)*0.7)
train <- df_balanced[train_idx, ]
test <- df_balanced[-train_idx, ]

table(train$MRJMDAYS)
table(test$MRJMDAYS)

# Fit classification tree
set.seed(123) 
mrj_tree <- tree(MRJMDAYS ~ ., data = train)

# Print and plot the tree
summary(mrj_tree)
plot(mrj_tree)
text(mrj_tree, pretty = 0)

# predict on test
tree_pred <- predict(mrj_tree, newdata = test, type = "class")

# confusion matrix
table(Predicted = tree_pred, Actual = test$MRJMDAYS)

# accuracy
mean(tree_pred == test$MRJMDAYS)
```

After analyzing that most predictions were made for either classes 1 or 5, re-framing the problem might help improve the predictive power of classification trees. Collapsing the categorizes into fewer and more balanced classes, like never used marijuana, occasional use, and frequent use, can help the model pick up more nuances of the data.

To further improve predictions beyond a simple muti-class classification tree, we might consider ensemble methods that can take multiple classification trees and aggregate their solutions to produce a more robust prediction. In this specific case, a random forest method would be helpful in reducing variance of the current model by creating many classification trees using random sampling with replacement, while also limiting the number of variables at each split so our model captures of the nuances in the data.

The following code utilizes the random forest method on a balanced data set to predict the frequency of marijuana use based on the same predictor variables.

The trade off when conducting ensemble methods, is loosing the accessibility of analyzing a decision tree as random forest is unable to produce a visual of an aggregated tree. However the model can still be analyzed on other ways like looking at the error rate or accuracy and identifying significant predictor variables. The random forest model predicts significantly better than the simple classification tree, with an accuracy of 44.83% correct predictions, but still not a strong model. This is an improvement however, as the confusion matrix shows a fair prediction across all frequencies, even doing a good job predicting frequent and occasional users. This model definitely accounts for more of the nuances in the data and brings to light factors that affect frequency of marijuana use.

Variable importance of the random forest model will highlight demographic or socio-economic factors that may impact the frequency of youth marijuana use. Similarly to the binary classification model, grade level and student health are important factors that determine frequent use. This could be important to highlight in awareness classes that just because drug use may seem more prevalent in higher grades, does not mean giving into peer pressure is the right path forward. Also highlighting the importance of health, not just for high risk students, but also any potential risks that might occur with frequent drug use.

```{r}
### Random Forest
# regrouping data
df <- df %>%
  mutate(MRJMDAYS_CAT = case_when(
    MRJMDAYS == 5 ~ "Never",
    MRJMDAYS %in% c(1, 2) ~ "Occasional",
    MRJMDAYS %in% c(3, 4) ~ "Frequent"
  )) %>%
  mutate(MRJMDAYS_CAT = factor(MRJMDAYS_CAT))
table(df$MRJMDAYS_CAT)

# Define variables
vars <- c('IRSEX', 'NEWRACE2', 'HEALTH2', 'EDUSCHGRD2', 'INCOME', 'IMOTHER', 'PDEN10', 'MRJMDAYS_CAT')

# Subset and clean data
df_tree <- df %>%
  select(all_of(vars)) %>%
  drop_na() %>%
  mutate(MRJMDAYS_CAT = as.factor(MRJMDAYS_CAT))

# balance data
set.seed(123) 
df_balanced <- RandUnderClassif(MRJMDAYS_CAT ~ ., df_tree)
table(df_balanced$MRJMDAYS_CAT)

# train test split
set.seed(123)
train_idx <- sample(1:nrow(df_balanced), nrow(df_balanced)*0.7)
train <- df_balanced[train_idx, ]
test <- df_balanced[-train_idx, ]

# fit random forest model
set.seed(123) 
rf_tree <- randomForest(MRJMDAYS_CAT ~., data = df_balanced[train_idx,], mtry = 3, importance = TRUE)
rf_tree

# importance
importance(rf_tree)
varImpPlot(rf_tree)

# predict on test data
yhat.bag <- predict(rf_tree, newdata = test)
plot(yhat.bag, test$MRJMDAYS_CAT) # plot points
abline(0, 1) # plot regression line

# accuracy
mean(yhat.bag == test$MRJMDAYS_CAT)
```

## Multi-Class: \# days used marijuana in past year

When looking at long term marijuana usage in youths, it might be more effective to analyze predictors that can predict marijuana usage in a year. The following model includes all of the parameters used to analyze marijuana usage in a month, including fewer categories of the target variable, balanced data, and random forest model implementation. The accuracy is significantly higher at 86.95%, and its important to note that the model picks up on some of the nuances of the data but still heavily biased towards predicting no marijuana use among youths.

Despite the biased predictions of our model, the importance of the variables from this model can still be used to analyze, most likely what predicts no marijuana use among youths, which in turn can be helpful in practicing safe and preventative drug use lessons in school. For monthly usage models, grade level is still an important variable, which tells us educating youths about the repercussions of drug use at a younger age is becoming increasingly important. However, this model shows race and family income are also predictors for long term marijuana usage in youths. We might consider paying special attention towards families with high income that might be able to support the expenses of drug use, as well as communities that revolve around prevalent drug scenes.

```{r}
### Random Forest
# regrouping data
df <- df %>%
  mutate(MRJYDAYS_CAT = case_when(
    MRJYDAYS == 6 ~ "Never",
    MRJYDAYS %in% c(1, 2) ~ "Occasional",
    MRJYDAYS %in% c(3, 4, 5) ~ "Frequent"
  )) %>%
  mutate(MRJYDAYS_CAT = factor(MRJYDAYS_CAT))
table(df$MRJYDAYS_CAT)

# Define variables
vars <- c('IRSEX', 'NEWRACE2', 'HEALTH2', 'EDUSCHGRD2', 'INCOME', 'IMOTHER', 'PDEN10', 'MRJYDAYS_CAT')

# Subset and clean data
df_tree <- df %>%
  select(all_of(vars)) %>%
  drop_na() %>%
  mutate(MRJYDAYS_CAT = as.factor(MRJYDAYS_CAT))


# balance data
set.seed(123) 
df_balanced <- RandUnderClassif(MRJYDAYS_CAT ~ ., df_tree)
table(df_balanced$MRJYDAYS_CAT)

# train test split
set.seed(123)
train_idx <- sample(1:nrow(df_balanced), nrow(df_balanced)*0.7)
train <- df_balanced[train_idx, ]
test <- df_balanced[-train_idx, ]

# fit random forest model
set.seed(123) 
rf_tree <- randomForest(MRJYDAYS_CAT ~., data = train, mtry = 3, importance = TRUE)
rf_tree

# importance
importance(rf_tree)
varImpPlot(rf_tree)

# predict on test data
yhat.bag <- predict(rf_tree, newdata = test)
plot(yhat.bag, test$MRJYDAYS_CAT) # plot points
abline(0, 1) # plot regression line

# accuracy
mean(yhat.bag == test$MRJYDAYS_CAT)
```

## Regression Classification: Age of First Usage

A regression classification model that predicts the age in which youths first used marijuana could be useful in helping schools understand at what age or grade level to implement resources and awareness towards drug use.

The following model examines from only students who used marijuana, what factors can help predict the age of first use. From the regression tree, we can see that mainly grade level and race are prevalent factors in predicting age of first marijuana use. The rmse measures the average difference between predicted and actual values in years, in terms of this model means that the model can on average predict the age of a participants first use of marijuana within 1.8 years.

The regression model shows that on the younger side, youths are exposed to marijuana at around 11-12 years old, but 13-14 year olds are more typical perhaps. This result might incline schools to start drug use awareness and safety at around 5th grade and continue to emphasize the importance of awareness through middle and high school.

```{r}
### REGRESSION CLASSIFICATION
# clean target variable (IRMJAGE)
df <- df %>%
  filter(IRMJAGE != 991) %>%  # remove "never used"
  mutate(IRMJAGE = as.numeric(IRMJAGE)) %>%
  drop_na() 

# Define variables
vars <- c('IRSEX', 'NEWRACE2', 'HEALTH2', 'EDUSCHGRD2','INCOME', 'PDEN10', 'IRMJAGE')

  
# Subset and clean data
df_tree <- df %>%
  select(all_of(vars)) %>%
  drop_na()

# train test split
set.seed(123)
train_idx <- sample(1:nrow(df_tree), nrow(df_tree)*0.7)
train <- df_tree[train_idx, ]
test <- df_tree[-train_idx, ]

# fit regression classification
set.seed(123) 
reg_tree <- tree(IRMJAGE ~ ., data = train)
summary(reg_tree)

# plot regression tree
plot(reg_tree)
text(reg_tree, pretty = 0)

# prune tree
set.seed(123) 
cv_reg <- cv.tree(reg_tree)
plot(cv_reg$size, cv_reg$dev, type = "b")

# pruned tree
prune_reg <- prune.tree(reg_tree, best = 3)
plot(prune_reg)
text(prune_reg, pretty = 0)

# predict on tree
yhat <- predict(prune_reg, newdata = test)

# rmse
mse <- mean((yhat - test$IRMJAGE)^2)
sqrt(mse)
```

A boosting model can help learn additional nuances of the data, but with an rmse of about the same (1.76 years accuracy), the results are about the same as a simpler model. It would make more sense to use a regular regression classification tree in this scenario.

```{r}
### BOOSTING REGRESSION MODEL
# boosting model (regression)
library(gbm)
set.seed(123) 
boost_reg <- gbm(IRMJAGE ~ ., data = train,
    distribution = "gaussian", n.trees = 1000)
summary(boost_reg)

# predict on test data
yhat.boost <- predict(boost_reg,
    newdata = test, n.trees = 1000)

# mse
mse <- mean((yhat.boost - test$IRMJAGE)^2)
sqrt(mse)
```
